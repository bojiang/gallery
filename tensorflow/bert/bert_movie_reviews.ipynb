{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0a4mTk9o1Qg"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "# Predicting Movie Review Sentiment with [kpe/bert-for-tf2](https://github.com/kpe/bert-for-tf2)\n",
    "\n",
    "\n",
    "A modification of https://github.com/kpe/bert-for-tf2/blob/master/examples/gpu_movie_reviews.ipynb,\n",
    "which is a modification of https://github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb using the Tensorflow 2.0 Keras\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=tensorflow&ea=tensorflow_2_bert_movie_review&dt=tensorflow_2_bert_movie_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/envs/bentoml-dev-py36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tqdm bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.config.set_visible_devices([], 'GPU')  # disable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Evlk1N78HIXM",
    "outputId": "9ada0ad2-3297-414d-b7bb-748063302382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow:  2.2.0\n",
      "Python:  3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 21:14:29) \n",
      "[GCC 7.3.0]\n",
      "WARNING:tensorflow:From <ipython-input-4-7068fd7facab>:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow: \", tf.__version__)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"GPU: \", tf.test.is_gpu_available())\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 6  # required by clipper benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZtI7cKWDbUVc"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fom_ff20gyy6"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in tqdm(os.listdir(directory), desc=os.path.basename(directory)):\n",
    "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "        fname=\"aclImdb.tar.gz\", \n",
    "        origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "        extract=True)\n",
    "\n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                           \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                          \"aclImdb\", \"test\"))\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CaE2G_2DdzVg"
   },
   "source": [
    "Let's use the `MovieReviewData` class below, to prepare/encode \n",
    "the data for feeding into our BERT model, by:\n",
    "  - tokenizing the text\n",
    "  - trim or pad it to a `max_seq_len` length\n",
    "  - append the special tokens `[CLS]` and `[SEP]`\n",
    "  - convert the string tokens to numerical `ID`s using the original model's token encoding from `vocab.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abfwdn-g135"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "\n",
    "\n",
    "class MovieReviewData:\n",
    "    DATA_COLUMN = \"sentence\"\n",
    "    LABEL_COLUMN = \"polarity\"\n",
    "\n",
    "    def __init__(self, tokenizer: FullTokenizer, sample_size=None, max_seq_len=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_size = sample_size\n",
    "        self.max_seq_len = 0\n",
    "        train, test = download_and_load_datasets()\n",
    "        \n",
    "        train, test = map(lambda df: df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index), \n",
    "                          [train, test])\n",
    "                \n",
    "        if sample_size is not None:\n",
    "            train, test = train.head(sample_size), test.head(sample_size)\n",
    "            # train, test = map(lambda df: df.sample(sample_size), [train, test])\n",
    "        \n",
    "        ((self.train_x, self.train_y),\n",
    "         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "\n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        ((self.train_x, self.train_x_token_types),\n",
    "         (self.test_x, self.test_x_token_types)) = map(self._pad, \n",
    "                                                       [self.train_x, self.test_x])\n",
    "\n",
    "    def _prepare(self, df):\n",
    "        x, y = [], []\n",
    "        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
    "            for ndx, row in df.iterrows():\n",
    "                text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]\n",
    "                tokens = self.tokenizer.tokenize(text)\n",
    "                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "                x.append(token_ids)\n",
    "                y.append(int(label))\n",
    "                pbar.update()\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    def _pad(self, ids):\n",
    "        x, t = [], []\n",
    "        token_type_ids = [0] * self.max_seq_len\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "            t.append(token_type_ids)\n",
    "        return np.array(x), np.array(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGL0mEoNFGlP"
   },
   "source": [
    "## A tweak\n",
    "\n",
    "Because of a `tf.train.load_checkpoint` limitation requiring list permissions on the google storage bucket, we need to copy the pre-trained BERT weights locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw_F488eixTV"
   },
   "outputs": [],
   "source": [
    "asset_path = 'asset'\n",
    "bert_model_name = \"uncased_L-12_H-768_A-12\"\n",
    "bert_ckpt_dir    = os.path.join(asset_path, bert_model_name)\n",
    "bert_ckpt_file   = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
    "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -f asset/uncased_L-12_H-768_A-12.zip ]; then\n",
    "    curl -o asset/uncased_L-12_H-768_A-12.zip --create-dirs https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "fi\n",
    "if [ ! -d asset/uncased_L-12_H-768_A-12 ]; then\n",
    "    unzip asset/uncased_L-12_H-768_A-12.zip -d asset/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4xPTleh2X2b"
   },
   "source": [
    "# Preparing the Data\n",
    "\n",
    "Now let's fetch and prepare the data by taking the first `max_seq_len` tokenens after tokenizing with the BERT tokenizer, und use `sample_size` examples for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of about 2500 train and test examples, respectively, and use the first 128 tokens only (transformers memory and computation requirements scale quadraticly with the sequence length - so with a TPU you might use `max_seq_len=512`, but on a GPU this would be too slow, and you will have to use a very small `batch_size`s to fit the model into the GPU memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "kF_3KhGQ0GTc",
    "outputId": "fd993d23-61ce-4aae-c992-5b5591123198"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos: 100%|██████████| 12500/12500 [00:00<00:00, 34603.48it/s]\n",
      "neg: 100%|██████████| 12500/12500 [00:00<00:00, 37100.79it/s]\n",
      "pos: 100%|██████████| 12500/12500 [00:00<00:00, 37403.83it/s]\n",
      "neg: 100%|██████████| 12500/12500 [00:00<00:00, 37488.08it/s]\n",
      "100%|██████████| 2.56k/2.56k [00:02<00:00, 928it/s]  \n",
      "/opt/conda/envs/bentoml-dev-py36/lib/python3.6/site-packages/ipykernel_launcher.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "100%|██████████| 2.56k/2.56k [00:02<00:00, 959it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 178\n",
      "CPU times: user 18 s, sys: 4.16 s, total: 22.2 s\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "data = MovieReviewData(tokenizer, \n",
    "                       sample_size=10*128*2, #10*128*2\n",
    "                       max_seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "prRQM8pDi8xI",
    "outputId": "b98433d4-c7e6-4bb5-af54-941f52e0b8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            train_x (2560, 128)\n",
      "train_x_token_types (2560, 128)\n",
      "            train_y (2560,)\n",
      "             test_x (2560, 128)\n",
      "        max_seq_len 128\n"
     ]
    }
   ],
   "source": [
    "print(\"            train_x\", data.train_x.shape)\n",
    "print(\"train_x_token_types\", data.train_x_token_types.shape)\n",
    "print(\"            train_y\", data.train_y.shape)\n",
    "\n",
    "print(\"             test_x\", data.test_x.shape)\n",
    "\n",
    "print(\"        max_seq_len\", data.max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "## Adapter BERT\n",
    "\n",
    "If we decide to use [adapter-BERT](https://arxiv.org/abs/1902.00751) we need some helpers for freezing the original BERT layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "def flatten_layers(root_layer):\n",
    "    if isinstance(root_layer, keras.layers.Layer):\n",
    "        yield root_layer\n",
    "    for layer in root_layer._layers:\n",
    "        for sub_layer in flatten_layers(layer):\n",
    "            yield sub_layer\n",
    "\n",
    "\n",
    "def freeze_bert_layers(l_bert):\n",
    "    \"\"\"\n",
    "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
    "    \"\"\"\n",
    "    for layer in flatten_layers(l_bert):\n",
    "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
    "            layer.trainable = True\n",
    "        elif len(layer._layers) == 0:\n",
    "            layer.trainable = False\n",
    "        l_bert.embeddings_layer.trainable = False\n",
    "\n",
    "\n",
    "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                   end_learn_rate=1e-7,\n",
    "                                   warmup_epoch_count=10,\n",
    "                                   total_epoch_count=90):\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < warmup_epoch_count:\n",
    "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
    "        else:\n",
    "            res = max_learn_rate*math.exp(\n",
    "                math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
    "        return float(res)\n",
    "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "    return learning_rate_scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "# Creating a model\n",
    "\n",
    "Now let's create a classification model using [adapter-BERT](https//arxiv.org/abs/1902.00751), which is clever way of reducing the trainable parameter count, by freezing the original BERT weights, and adapting them with two FFN bottlenecks (i.e. `adapter_size` bellow) in every BERT layer.\n",
    "\n",
    "**N.B.** The commented out code below show how to feed a `token_type_ids`/`segment_ids` sequence (which is not needed in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(max_seq_len, adapter_size=64):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    #adapter_size = 64  # see - arXiv:1902.00751\n",
    "\n",
    "    # create the bert layer\n",
    "    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "        bc = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = map_stock_config_to_params(bc)\n",
    "        bert_params.adapter_size = adapter_size\n",
    "        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "    input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
    "    # token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"token_type_ids\")\n",
    "    # output         = bert([input_ids, token_type_ids])\n",
    "    output         = bert(input_ids)\n",
    "\n",
    "    print(\"bert shape\", output.shape)\n",
    "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
    "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "    logits = keras.layers.Dropout(0.5)(logits)\n",
    "    logits = keras.layers.Dense(units=2, activation=\"softmax\")(logits)\n",
    "\n",
    "    # model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)\n",
    "    # model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n",
    "    model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "    model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "    # load the pre-trained model weights\n",
    "    load_stock_weights(bert, bert_ckpt_file)\n",
    "\n",
    "    # freeze weights if adapter-BERT is used\n",
    "    if adapter_size is not None:\n",
    "        freeze_bert_layers(bert)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "id": "bZnmtDc7HlEm",
    "outputId": "fcd96c78-792c-4032-d188-73a9c21ec304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 128, 768)\n",
      "Done loading 196 BERT weights from: asset/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f1fcc5f2358> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1538      \n",
      "=================================================================\n",
      "Total params: 109,482,242\n",
      "Trainable params: 109,482,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adapter_size = None # use None to fine-tune all of BERT\n",
    "model = create_model(data.max_seq_len, adapter_size=adapter_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZuLOkwonF-9S",
    "outputId": "ce1451d4-310c-41f1-ddd3-a2e0841d8528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 5.000000000000001e-07.\n",
      "Epoch 1/2\n",
      "192/192 [==============================] - 118s 615ms/step - loss: 0.7203 - acc: 0.5191 - val_loss: 0.7058 - val_acc: 0.5312 - lr: 5.0000e-07\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 1.0000000000000002e-06.\n",
      "Epoch 2/2\n",
      "192/192 [==============================] - 116s 606ms/step - loss: 0.7004 - acc: 0.5460 - val_loss: 0.6859 - val_acc: 0.5312 - lr: 1.0000e-06\n",
      "CPU times: user 4min 56s, sys: 40.5 s, total: 5min 36s\n",
      "Wall time: 4min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1f6c03b0b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "log_dir = \".log/movie_reviews/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "total_epoch_count = 2\n",
    "model.fit(x=data.train_x, y=data.train_y,\n",
    "          validation_split=0.1,\n",
    "          batch_size=12,\n",
    "          shuffle=True,\n",
    "          epochs=total_epoch_count,\n",
    "          callbacks=[create_learning_rate_scheduler(max_learn_rate=1e-5,\n",
    "                                                    end_learn_rate=1e-7,\n",
    "                                                    warmup_epoch_count=20,\n",
    "                                                    total_epoch_count=total_epoch_count),\n",
    "                     keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "                     tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./movie_reviews.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "BSqMu64oHzqy",
    "outputId": "95a8284b-b3b2-4a7d-f335-c4ff65f8f920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 32s 398ms/step - loss: 0.6780 - acc: 0.5617\n",
      "80/80 [==============================] - 32s 399ms/step - loss: 0.6985 - acc: 0.5410\n",
      "train acc 0.561718761920929\n",
      " test acc 0.541015625\n",
      "CPU times: user 1min 6s, sys: 28.4 ms, total: 1min 6s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "print(\"train acc\", train_acc)\n",
    "print(\" test acc\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSKDZEnVabnl"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the trained model, let's load the saved weights in a new model instance, and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 128, 768)\n",
      "Done loading 196 BERT weights from: asset/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7fa1f1e7cb38> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1538      \n",
      "=================================================================\n",
      "Total params: 109,482,242\n",
      "Trainable params: 109,482,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(data.max_seq_len, adapter_size=None)\n",
    "model.load_weights(\"./movie_reviews.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "id": "qCpabQ15WS3U",
    "outputId": "220b2b55-dd78-4795-d3b6-d364c8323734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 31s 390ms/step - loss: 0.6986 - acc: 0.5406\n",
      " test acc 0.5406249761581421\n",
      "CPU times: user 33.7 s, sys: 124 ms, total: 33.8 s\n",
      "Wall time: 33.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# _, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "# print(\"train acc\", train_acc)\n",
    "print(\" test acc\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5uzdOFQ5awM1"
   },
   "source": [
    "# Prediction\n",
    "\n",
    "For prediction, we need to prepare the input text the same way as we did for training - tokenize, adding the special `[CLS]` and `[SEP]` token at begin and end of the token sequence, and pad to match the model input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 97 ms, sys: 4.02 ms, total: 101 ms\n",
      "Wall time: 109 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['positive', 'positive', 'positive', 'positive']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "CLASSES = [\"negative\",\"positive\"]\n",
    "max_seq_len = 128\n",
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\",\n",
    "]\n",
    "\n",
    "inputs = pd.DataFrame(pred_sentences)\n",
    "\n",
    "pred_tokens    = map(tokenizer.tokenize, inputs.to_numpy()[:, 0].tolist())\n",
    "pred_tokens    = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
    "pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
    "pred_token_ids = map(lambda tids: tids + [0] * (max_seq_len-len(tids)), pred_token_ids)\n",
    "pred_token_ids = np.array(list(pred_token_ids))\n",
    "\n",
    "res = model(pred_token_ids).numpy().argmax(axis=-1)\n",
    "[CLASSES[i] for i in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build & Save bentoml service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bentoml_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bentoml_service.py\n",
    "\n",
    "import bentoml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "from bentoml.frameworks.tensorflow import TensorflowSavedModelArtifact\n",
    "from bentoml.service.artifacts.common import PickleArtifact\n",
    "from bentoml.adapters import DataframeInput\n",
    "\n",
    "\n",
    "CLASSES = [\"negative\",\"positive\"]\n",
    "max_seq_len = 128\n",
    "\n",
    "try:\n",
    "    tf.config.set_visible_devices([], 'GPU')  # disable GPU, required when served in docker\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "@bentoml.env(pip_dependencies=['tensorflow', 'bert-for-tf2'])\n",
    "@bentoml.artifacts([TensorflowSavedModelArtifact('model'), PickleArtifact('tokenizer')])\n",
    "class Service(bentoml.BentoService):\n",
    "\n",
    "    def tokenize(self, inputs: pd.DataFrame):\n",
    "        tokenizer = self.artifacts.tokenizer\n",
    "        if isinstance(inputs, pd.DataFrame):\n",
    "            inputs = inputs.to_numpy()[:, 0].tolist()\n",
    "        else:\n",
    "            inputs = inputs.tolist()  # for predict_clipper\n",
    "        pred_tokens = map(tokenizer.tokenize, inputs)\n",
    "        pred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
    "        pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
    "        pred_token_ids = map(lambda tids: tids + [0] * (max_seq_len - len(tids)), pred_token_ids)\n",
    "        pred_token_ids = tf.constant(list(pred_token_ids), dtype=tf.int32)\n",
    "        return pred_token_ids\n",
    "\n",
    "    @bentoml.api(input=DataframeInput(), mb_max_latency=3000, mb_max_batch_size=20, batch=True)\n",
    "    def predict(self, inputs: pd.DataFrame) -> List[str]:\n",
    "        model = self.artifacts.model\n",
    "        pred_token_ids = self.tokenize(inputs)\n",
    "        res = model(pred_token_ids).numpy().argmax(axis=-1)\n",
    "        return [CLASSES[i] for i in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-23 01:16:19,822] WARNING - Importing from \"bentoml.artifact.*\" has been deprecated. Instead, use`bentoml.frameworks.*` and `bentoml.service.*`. e.g.:, `from bentoml.frameworks.sklearn import SklearnModelArtifact`, `from bentoml.service.artifacts import BentoServiceArtifact`, `from bentoml.service.artifacts.common import PickleArtifact`\n",
      "[2020-09-23 01:16:19,914] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2020-09-23 01:16:19,956] WARNING - pip package requirement tensorflow already exist\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7fa1f1e1fcf8>, because it is not built.\n",
      "WARNING:tensorflow:From /opt/conda/envs/bentoml-dev-py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /tmp/bentoml-temp-p0s_q6re/Service/artifacts/model_saved_model/assets\n",
      "[2020-09-23 01:17:16,410] INFO - Detected non-PyPI-released BentoML installed, copying local BentoML modulefiles to target saved bundle path..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/bentoml-dev-py36/lib/python3.6/site-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  \"Distutils was imported before Setuptools. This usage is discouraged \"\n",
      "/opt/conda/envs/bentoml-dev-py36/lib/python3.6/site-packages/setuptools/dist.py:458: UserWarning: Normalizing '0.9.0.pre+5.g893a472' to '0.9.0rc0+5.g893a472'\n",
      "  warnings.warn(tmpl.format(**locals()))\n",
      "warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "warning: no previously-included files matching '*.pyo' found anywhere in distribution\n",
      "warning: no previously-included files matching '.git' found anywhere in distribution\n",
      "warning: no previously-included files matching '.ipynb_checkpoints' found anywhere in distribution\n",
      "warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
      "warning: no directories found matching 'bentoml/yatai/web/dist'\n",
      "no previously-included directories found matching 'e2e_tests'\n",
      "no previously-included directories found matching 'tests'\n",
      "no previously-included directories found matching 'benchmark'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING BentoML-0.9.0rc0+5.g893a472/bentoml/_version.py\n",
      "set BentoML-0.9.0rc0+5.g893a472/bentoml/_version.py to '0.9.0.pre+5.g893a472'\n",
      "[2020-09-23 01:17:17,824] INFO - BentoService bundle 'Service:20200923011620_332663' saved to: /home/bentoml/bentoml/repository/Service/20200923011620_332663\n"
     ]
    }
   ],
   "source": [
    "from bentoml_service import Service\n",
    "\n",
    "bento_svc = Service()\n",
    "bento_svc.pack(\"model\", model)\n",
    "bento_svc.pack(\"tokenizer\", tokenizer)\n",
    "saved_path = bento_svc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving\n",
    "\n",
    "Since BERT is a large model, if you met OOM bellow,  \n",
    "you may need to restart this kernel to release the RAM/GRAM used by training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bentoml serve-gunicorn Service:latest --port 5000 --enable-microbatch --workers 1\n",
      "[2020-09-23 01:17:27,340] INFO - Getting latest version Service:20200923011620_332663\n",
      "[2020-09-23 01:17:27,340] INFO - Starting BentoML API server in production mode..\n",
      "[2020-09-23 01:17:27,478] INFO - Running micro batch service on :5000\n",
      "[2020-09-23 01:17:27 +0800] [11601] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-09-23 01:17:27 +0800] [11589] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-09-23 01:17:27 +0800] [11589] [INFO] Listening at: http://0.0.0.0:39047 (11589)\n",
      "[2020-09-23 01:17:27 +0800] [11601] [INFO] Listening at: http://0.0.0.0:5000 (11601)\n",
      "[2020-09-23 01:17:27 +0800] [11601] [INFO] Using worker: aiohttp.worker.GunicornWebWorker\n",
      "[2020-09-23 01:17:27 +0800] [11589] [INFO] Using worker: sync\n",
      "[2020-09-23 01:17:27 +0800] [11603] [INFO] Booting worker with pid: 11603\n",
      "[2020-09-23 01:17:27 +0800] [11602] [INFO] Booting worker with pid: 11602\n",
      "[2020-09-23 01:17:27,520] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2020-09-23 01:17:27,536] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.9.0.pre, but loading from BentoML version 0.9.0.pre+5.g893a472\n",
      "[2020-09-23 01:17:27,538] INFO - Micro batch enabled for API `predict`\n",
      "[2020-09-23 01:17:27,538] INFO - Your system nofile limit is 10000, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "[2020-09-23 01:17:28,523] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2020-09-23 01:17:28,539] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.9.0.pre, but loading from BentoML version 0.9.0.pre+5.g893a472\n",
      "[2020-09-23 01:17:29,929] WARNING - Importing from \"bentoml.artifact.*\" has been deprecated. Instead, use`bentoml.frameworks.*` and `bentoml.service.*`. e.g.:, `from bentoml.frameworks.sklearn import SklearnModelArtifact`, `from bentoml.service.artifacts import BentoServiceArtifact`, `from bentoml.service.artifacts.common import PickleArtifact`\n",
      "2020-09-23 01:17:29.933746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-09-23 01:17:29.936380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-09-23 01:17:29.936637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1060 computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2020-09-23 01:17:29.936788: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-09-23 01:17:29.938092: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-09-23 01:17:29.939343: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-09-23 01:17:29.939573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-09-23 01:17:29.941007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-09-23 01:17:29.941830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-09-23 01:17:29.944940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-09-23 01:17:29.945098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-09-23 01:17:29.945435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-09-23 01:17:29.945674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2020-09-23 01:17:30.884470: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2020-09-23 01:17:30.889082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-09-23 01:17:30.889107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      \n",
      "2020-09-23 01:17:30.909039: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2699905000 Hz\n",
      "2020-09-23 01:17:30.909280: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562575b1f060 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-09-23 01:17:30.909298: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "^C\n",
      "[2020-09-23 01:17:38 +0800] [11601] [INFO] Handling signal: int\n",
      "[2020-09-23 01:17:38 +0800] [11589] [INFO] Handling signal: int\n",
      "[2020-09-23 01:17:38 +0800] [11602] [INFO] Worker exiting (pid: 11602)\n",
      "[2020-09-23 01:17:38 +0800] [11601] [INFO] Shutting down: Master\n",
      "[2020-09-23 01:17:38 +0800] [11603] [INFO] Worker exiting (pid: 11603)\n"
     ]
    }
   ],
   "source": [
    "# Option 1: serve directly\n",
    "print(f\"bentoml serve-gunicorn Service:latest --port 5000 --enable-microbatch --workers 1\")\n",
    "!bentoml serve-gunicorn Service:latest --port 5000 --enable-microbatch --workers 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-23 02:04:26,010] INFO - Getting latest version Service:20200923011620_332663\n",
      "\u001b[39mFound Bento: /home/bentoml/bentoml/repository/Service/20200923011620_332663\u001b[0m\n",
      "[2020-09-23 02:04:26,029] WARNING - Using BentoML installed in `editable` model, the local BentoML repository including all code changes will be packaged together with saved bundle created, under the './bundled_pip_dependencies' directory of the saved bundle.\n",
      "[2020-09-23 02:04:26,045] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.9.0.pre, but loading from BentoML version 0.9.0.pre+7.g8af1c8b\n",
      "Building Docker image service:latest from Service:latest \n",
      "-\u001b[39mStep 1/15 : FROM bentoml/model-server:0.9.0.pre-py36\u001b[0m\n",
      "\u001b[39m ---> 4aac43d10e50\u001b[0m\n",
      "\u001b[39mStep 2/15 : ARG EXTRA_PIP_INSTALL_ARGS=\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 790054f5ad85\u001b[0m\n",
      "\u001b[39mStep 3/15 : ENV EXTRA_PIP_INSTALL_ARGS $EXTRA_PIP_INSTALL_ARGS\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 85b0a1b40542\u001b[0m\n",
      "\u001b[39mStep 4/15 : COPY environment.yml requirements.txt setup.sh* bentoml-init.sh python_version* /bento/\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 12a9ca2bcbb1\u001b[0m\n",
      "\u001b[39mStep 5/15 : WORKDIR /bento\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> aaf08cf8ce1d\u001b[0m\n",
      "\u001b[39mStep 6/15 : RUN chmod +x /bento/bentoml-init.sh\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> bf200a95297d\u001b[0m\n",
      "\u001b[39mStep 7/15 : RUN if [ -f /bento/bentoml-init.sh ]; then bash -c /bento/bentoml-init.sh; fi\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 57edb58d5f7f\u001b[0m\n",
      "\u001b[39mStep 8/15 : COPY . /bento\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> c437cdc1e846\u001b[0m\n",
      "\u001b[39mStep 9/15 : RUN if [ -d /bento/bundled_pip_dependencies ]; then pip install -U bundled_pip_dependencies/* ;fi\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> ebdad6995d1a\u001b[0m\n",
      "\u001b[39mStep 10/15 : ENV PORT 5000\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> d6a55db6b34a\u001b[0m\n",
      "\u001b[39mStep 11/15 : EXPOSE $PORT\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 27155e6e0f53\u001b[0m\n",
      "\u001b[39mStep 12/15 : COPY docker-entrypoint.sh /usr/local/bin/\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 2056b2a7d332\u001b[0m\n",
      "\u001b[39mStep 13/15 : RUN chmod +x /usr/local/bin/docker-entrypoint.sh\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> c12fa907ad46\u001b[0m\n",
      "\u001b[39mStep 14/15 : ENTRYPOINT [ \"docker-entrypoint.sh\" ]\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> a981e327955e\u001b[0m\n",
      "\u001b[39mStep 15/15 : CMD [\"bentoml\", \"serve-gunicorn\", \"/bento\"]\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 72e5cfef2904\u001b[0m\n",
      "\u001b[39m[Warning] One or more build-args [--quiet] were not consumed\u001b[0m\n",
      "\u001b[39mSuccessfully built 72e5cfef2904\u001b[0m\n",
      "\u001b[39mSuccessfully tagged service:latest\u001b[0m\n",
      "\u001b[32mFinished building service:latest from Service:latest\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Option 2: serve in docker\n",
    "!bentoml containerize Service:latest -t service:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1219fa944af2ae7dc82258130e974d4c717d8bd6dd71d6342377b9a3799ffa00\n"
     ]
    }
   ],
   "source": [
    " # launch docker instances\n",
    "!docker run -itd -p 5000:5000 service:latest --workers 1 --enable-microbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the API with requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[\"positive\"]'\n",
      "CPU times: user 3.01 ms, sys: 223 µs, total: 3.23 ms\n",
      "Wall time: 176 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "server_url = f\"http://127.0.0.1:5000/predict\"\n",
    "method = \"POST\"\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "pred_sentences =  [\"The acting was a bit lacking.\"]\n",
    "data = pd.DataFrame(pred_sentences).to_json()\n",
    "\n",
    "r = requests.request(method, server_url, headers=headers, data=data)\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Movie Reviews with bert-for-tf2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "bentoml-py36",
   "language": "python",
   "name": "bentoml-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
